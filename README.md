
# Scaling Techniques in Machine Learning Preprocessing

This repository accompanies a research project focused on analyzing the impact of various data scaling techniques on machine learning model performance.

## ğŸ“š Overview

The study evaluates multiple preprocessing scalers (e.g., StandardScaler, MinMaxScaler, RobustScaler) across a range of datasets and machine learning algorithms. The results provide insights into how different scaling methods affect accuracy, precision, recall, and model stability.

## ğŸ“ Repository Structure

```
â”œâ”€â”€ ScalingTechnique.ipynb       # Main Jupyter notebook with all code and analysis
â”œâ”€â”€ requirements.txt             # Python package dependencies
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Iris.csv
â”‚   â”œâ”€â”€ Housing.csv
â”‚   â”œâ”€â”€ winequality-red.csv
â”‚   â”œâ”€â”€ Breast_cancer_dataset.csv
```

## ğŸ“Š Datasets Used

- **Iris Dataset**
- **Wine Quality Dataset** (Red)
- **Breast Cancer Wisconsin Dataset**
- **California Housing Dataset**

## ğŸ” Key Features

- Comparative performance of ML models with different scalers
- Visualizations of scaled feature distributions
- Evaluation using metrics: accuracy, precision, recall
- Includes conclusions and recommendations for scaler selection

## ğŸš€ How to Run

1. Clone the repository
2. Install dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3. Run the notebook:
    ```bash
    jupyter notebook ScalingTechnique.ipynb
    ```

## ğŸ“Œ Results Summary

Scaling techniques significantly influence model behavior. No single scaler performs best across all datasets, highlighting the importance of dataset characteristics in preprocessing choices.

## ğŸ“ License

This repository is open for academic and educational use under the MIT License.

## ğŸ“« Contact

For any questions or feedback, feel free to reach out or open an issue.

